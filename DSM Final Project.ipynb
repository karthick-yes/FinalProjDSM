{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Business.json cleaing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json('yelp_academic_dataset_business.json', encoding='utf-8', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>stars</th>\n",
       "      <th>review_count</th>\n",
       "      <th>is_open</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>150346.000000</td>\n",
       "      <td>150346.000000</td>\n",
       "      <td>150346.000000</td>\n",
       "      <td>150346.000000</td>\n",
       "      <td>150346.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>36.671150</td>\n",
       "      <td>-89.357339</td>\n",
       "      <td>3.596724</td>\n",
       "      <td>44.866561</td>\n",
       "      <td>0.79615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.872759</td>\n",
       "      <td>14.918502</td>\n",
       "      <td>0.974421</td>\n",
       "      <td>121.120136</td>\n",
       "      <td>0.40286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>27.555127</td>\n",
       "      <td>-120.095137</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>32.187293</td>\n",
       "      <td>-90.357810</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>38.777413</td>\n",
       "      <td>-86.121179</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>39.954036</td>\n",
       "      <td>-75.421542</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>53.679197</td>\n",
       "      <td>-73.200457</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>7568.000000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            latitude      longitude          stars   review_count  \\\n",
       "count  150346.000000  150346.000000  150346.000000  150346.000000   \n",
       "mean       36.671150     -89.357339       3.596724      44.866561   \n",
       "std         5.872759      14.918502       0.974421     121.120136   \n",
       "min        27.555127    -120.095137       1.000000       5.000000   \n",
       "25%        32.187293     -90.357810       3.000000       8.000000   \n",
       "50%        38.777413     -86.121179       3.500000      15.000000   \n",
       "75%        39.954036     -75.421542       4.500000      37.000000   \n",
       "max        53.679197     -73.200457       5.000000    7568.000000   \n",
       "\n",
       "            is_open  \n",
       "count  150346.00000  \n",
       "mean        0.79615  \n",
       "std         0.40286  \n",
       "min         0.00000  \n",
       "25%         1.00000  \n",
       "50%         1.00000  \n",
       "75%         1.00000  \n",
       "max         1.00000  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of reviews across businesses: 44.87\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load business.json\n",
    "def load_business_json(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Load data\n",
    "business_df = load_business_json(\"/Users/vinayakgoswamy/Downloads/Yelp JSON/yelp_dataset/yelp_academic_dataset_business.json\")\n",
    "\n",
    "# Drop 'postal_code' if present\n",
    "if 'postal_code' in business_df.columns:\n",
    "    business_df.drop(columns=['postal_code'], inplace=True)\n",
    "\n",
    "# Drop rows missing review count\n",
    "business_df = business_df.dropna(subset=['name', 'stars', 'review_count'])\n",
    "\n",
    "# Show average number of reviews\n",
    "avg_reviews = business_df['review_count'].mean()\n",
    "print(f\"Average number of reviews across businesses: {avg_reviews:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out businesses with fewer than 15 reviews\n",
    "filtered_business_df = business_df[business_df['review_count'] >= 5]\n",
    "\n",
    "# Reset index for cleanliness\n",
    "filtered_business_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example path\n",
    "output_path = \"/Users/vinayakgoswamy/Desktop/DSM Final Project/cleaned_business.csv\"\n",
    "\n",
    "filtered_business_df.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "business_id,name,address,city,state,latitude,longitude,stars,review_count,is_open,attributes,categories,hours\n",
      "mpf3x-BjTdTEA3yCZrAYPw,The UPS Store,87 Grasso Plaza Shopping Center,Affton,MO,38.551126,-90.335695,3.0,15,1,{'BusinessAcceptsCreditCards': 'True'},\"Shipping Centers, Local Services, Notaries, Mailbox Centers, Printing Services\",\"{'Monday': '0:0-0:0', 'Tuesday': '8:0-18:30', 'Wednesday': '8:0-18:30', 'Thursday': '8:0-18:30', 'Friday': '8:0-18:30', 'Saturday': '8:0-14:0'}\"\n",
      "tUFrWirKiKi_TAnsVWINQQ,Target,5255 E Broadway Blvd,Tucson,AZ,32.223236,-110.880452,3.5,22,0,\"{'BikeParking': 'True', 'BusinessAcceptsCreditCards': 'True', 'RestaurantsPriceRange2': '2', 'CoatCheck': 'False', 'RestaurantsTakeOut': 'False', 'RestaurantsDelivery': 'False', 'Caters': 'False', 'WiFi': \"\"u'no'\"\", 'BusinessParking': \"\"{'garage': False, 'street': False, 'validated': False, 'lot': True, 'valet': False}\"\", 'WheelchairAccessible': 'True', 'HappyHour': 'False', 'OutdoorSeating': 'False', 'HasTV': 'False',\n"
     ]
    }
   ],
   "source": [
    "csv_string = filtered_business_df.to_csv(index=False)\n",
    "print(csv_string[:1000])  # Show first 1000 characters so you can copy and preview\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              business_id                      name  \\\n",
      "0  mpf3x-BjTdTEA3yCZrAYPw             The UPS Store   \n",
      "1  tUFrWirKiKi_TAnsVWINQQ                    Target   \n",
      "2  MTSW4McQd7CbVtyjqoe9mw        St Honore Pastries   \n",
      "3  mWMc6_wTdE0EUBKIGXDVfA  Perkiomen Valley Brewery   \n",
      "4  n_0UpQx1hsNbnPUSlodU8w           Famous Footwear   \n",
      "\n",
      "                                      address          city state   latitude  \\\n",
      "0             87 Grasso Plaza Shopping Center        Affton    MO  38.551126   \n",
      "1                        5255 E Broadway Blvd        Tucson    AZ  32.223236   \n",
      "2                                 935 Race St  Philadelphia    PA  39.955505   \n",
      "3                               101 Walnut St    Green Lane    PA  40.338183   \n",
      "4  8522 Eager Road, Dierbergs Brentwood Point     Brentwood    MO  38.627695   \n",
      "\n",
      "    longitude  stars  review_count  is_open  \\\n",
      "0  -90.335695    3.0            15        1   \n",
      "1 -110.880452    3.5            22        0   \n",
      "2  -75.155564    4.0            80        1   \n",
      "3  -75.471659    4.5            13        1   \n",
      "4  -90.340465    2.5            13        1   \n",
      "\n",
      "                                          attributes  \\\n",
      "0             {'BusinessAcceptsCreditCards': 'True'}   \n",
      "1  {'BikeParking': 'True', 'BusinessAcceptsCredit...   \n",
      "2  {'RestaurantsDelivery': 'False', 'OutdoorSeati...   \n",
      "3  {'BusinessAcceptsCreditCards': 'True', 'Wheelc...   \n",
      "4  {'BusinessAcceptsCreditCards': 'True', 'Restau...   \n",
      "\n",
      "                                          categories  \\\n",
      "0  Shipping Centers, Local Services, Notaries, Ma...   \n",
      "1  Department Stores, Shopping, Fashion, Home & G...   \n",
      "2  Restaurants, Food, Bubble Tea, Coffee & Tea, B...   \n",
      "3                          Brewpubs, Breweries, Food   \n",
      "4  Sporting Goods, Fashion, Shoe Stores, Shopping...   \n",
      "\n",
      "                                               hours  \n",
      "0  {'Monday': '0:0-0:0', 'Tuesday': '8:0-18:30', ...  \n",
      "1  {'Monday': '8:0-22:0', 'Tuesday': '8:0-22:0', ...  \n",
      "2  {'Monday': '7:0-20:0', 'Tuesday': '7:0-20:0', ...  \n",
      "3  {'Wednesday': '14:0-22:0', 'Thursday': '16:0-2...  \n",
      "4  {'Monday': '0:0-0:0', 'Tuesday': '10:0-18:0', ...  \n"
     ]
    }
   ],
   "source": [
    "# Load the CSV back into a DataFrame to verify it\n",
    "test_df = pd.read_csv(\"/Users/vinayakgoswamy/Desktop/DSM Final Project/cleaned_business.csv\")\n",
    "print(test_df.head())  # Show the first few rows to check the content\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "review.json Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total filtered reviews loaded: 6641275\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load cleaned business IDs (from your already cleaned business_df)\n",
    "business_df = pd.read_csv(\"/Users/vinayakgoswamy/Desktop/DSM Final Project/cleaned_business.csv\")\n",
    "valid_business_ids = set(business_df['business_id'])\n",
    "\n",
    "# Load review.json in chunks to avoid memory issues\n",
    "review_data = []\n",
    "with open(\"/Users/vinayakgoswamy/Downloads/Yelp JSON/yelp_dataset/yelp_academic_dataset_review.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        review = json.loads(line)\n",
    "        # Filter directly while reading\n",
    "        if review['business_id'] in valid_business_ids:\n",
    "            review_data.append(review)\n",
    "\n",
    "# Convert to DataFrame\n",
    "review_df = pd.DataFrame(review_data)\n",
    "print(\"Total filtered reviews loaded:\", len(review_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned review count: 6640275\n"
     ]
    }
   ],
   "source": [
    "# Drop rows with missing critical fields\n",
    "review_df.dropna(subset=['review_id', 'user_id', 'business_id', 'stars', 'text'], inplace=True)\n",
    "\n",
    "\n",
    "# Drop duplicates\n",
    "review_df.drop_duplicates(subset='review_id', inplace=True)\n",
    "\n",
    "# Remove reviews with very short text\n",
    "review_df['text'] = review_df['text'].astype(str)\n",
    "review_df = review_df[review_df['text'].str.strip().str.len() >= 30]\n",
    "\n",
    "\n",
    "print(\"Cleaned review count:\", len(review_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_df.to_csv(\"/Users/vinayakgoswamy/Desktop/DSM Final Project/cleaned_review.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned review count: 3355929\n"
     ]
    }
   ],
   "source": [
    "review_df_further = review_df[~((review_df['useful'] == 0) & \n",
    "                        (review_df['funny'] == 0) & \n",
    "                        (review_df['cool'] == 0))]\n",
    "\n",
    "print(\"Cleaned review count:\", len(review_df_further))\n",
    "\n",
    "\n",
    "review_df_further.to_csv(\"/Users/vinayakgoswamy/Desktop/DSM Final Project/further_cleaned_review.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                review_id                 user_id             business_id  \\\n",
      "0  KU_O5udG6zpxOg-VcAEodg  mh_-eMZ6K5RLWhZyISBhwA  XQfwVwDr-v0ZS3_CbbE5Xw   \n",
      "1  BiTunyQ73aT9WBnpR9DZGw  OyoGAe7OKpv6SyGZT5g77Q  7ATYjTIgM3jUlt4UM3IypQ   \n",
      "2  saUsX_uimxRlCVr67Z4Jig  8g_iMtfSiwikVnbP2etR0A  YjUWPpI6HXG530lwP-fb2A   \n",
      "3  AqPFMleE6RsU23_auESxiA  _7bHUi9Uuf5__HHc_Q8guQ  kxX2SOes4o-D3ZQBkiMRfA   \n",
      "4  Sx8TMOWLNuJBWer-0pcmoA  bcjbaE6dDog4jkNY91ncLQ  e4Vwtrqf-wpJfwesgvdgxQ   \n",
      "\n",
      "   stars  useful  funny  cool  \\\n",
      "0    3.0       0      0     0   \n",
      "1    5.0       1      0     1   \n",
      "2    3.0       0      0     0   \n",
      "3    5.0       1      0     1   \n",
      "4    4.0       1      0     1   \n",
      "\n",
      "                                                text                 date  \n",
      "0  If you decide to eat here, just be aware it is...  2018-07-07 22:09:11  \n",
      "1  I've taken a lot of spin classes over the year...  2012-01-03 15:28:18  \n",
      "2  Family diner. Had the buffet. Eclectic assortm...  2014-02-05 20:30:30  \n",
      "3  Wow!  Yummy, different,  delicious.   Our favo...  2015-01-04 00:01:03  \n",
      "4  Cute interior and owner (?) gave us tour of up...  2017-01-14 20:54:15  \n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv(\"/Users/vinayakgoswamy/Desktop/DSM Final Project/cleaned_review.csv\", nrows=50)\n",
    "print(test_df.head()) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkin. json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>---kPU91CF4Lq2-WlRu9Lw</td>\n",
       "      <td>2020-03-13 21:10:56, 2020-06-02 22:18:06, 2020...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>--0iUa4sNDFiZFrAdIWhZQ</td>\n",
       "      <td>2010-09-13 21:43:09, 2011-05-04 23:08:15, 2011...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>--7PUidqRWpRSpXebiyxTg</td>\n",
       "      <td>2011-02-15 17:12:00, 2011-07-28 02:46:10, 2012...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>--7jw19RH9JKXgFohspgQw</td>\n",
       "      <td>2014-04-21 20:42:11, 2014-04-28 21:04:46, 2014...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>--8IbOsAAxjKRoYsBFL-PA</td>\n",
       "      <td>2015-06-06 01:03:19, 2015-07-29 16:50:58, 2015...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id                                               date\n",
       "0  ---kPU91CF4Lq2-WlRu9Lw  2020-03-13 21:10:56, 2020-06-02 22:18:06, 2020...\n",
       "1  --0iUa4sNDFiZFrAdIWhZQ  2010-09-13 21:43:09, 2011-05-04 23:08:15, 2011...\n",
       "3  --7PUidqRWpRSpXebiyxTg  2011-02-15 17:12:00, 2011-07-28 02:46:10, 2012...\n",
       "4  --7jw19RH9JKXgFohspgQw  2014-04-21 20:42:11, 2014-04-28 21:04:46, 2014...\n",
       "5  --8IbOsAAxjKRoYsBFL-PA  2015-06-06 01:03:19, 2015-07-29 16:50:58, 2015..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load checkin data\n",
    "with open(\"/Users/vinayakgoswamy/Downloads/Yelp JSON/yelp_dataset/yelp_academic_dataset_checkin.json\", \"r\") as f:\n",
    "    checkin_data = [json.loads(line) for line in f]\n",
    "\n",
    "checkin_df = pd.DataFrame(checkin_data)\n",
    "\n",
    "# Drop rows with missing business_id\n",
    "checkin_df = checkin_df.dropna(subset=[\"business_id\"])\n",
    "\n",
    "\n",
    "# Filter checkin data to keep only those with valid business_id\n",
    "filtered_checkin_df = checkin_df[checkin_df[\"business_id\"].isin(valid_business_ids)]\n",
    "\n",
    "\n",
    "\n",
    "# Save to CSV (or preview in memory)\n",
    "filtered_checkin_df.to_csv(\"/Users/vinayakgoswamy/Desktop/DSM Final Project/cleaned_checkin.csv\", index=False)\n",
    "\n",
    "# Preview\n",
    "filtered_checkin_df.head()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tips.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>compliment_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>tA1U-XSh9woo73eQmWGyAQ</td>\n",
       "      <td>xHwvbm1SJwtaZtOZzFQcmQ</td>\n",
       "      <td>If you haven't  been here in a good while or i...</td>\n",
       "      <td>2016-06-11 23:18:23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>nhXyjGOCfi-EW09bJqfAzQ</td>\n",
       "      <td>1Efad30BdOeqqjX2d6P4sw</td>\n",
       "      <td>You MUST have the popover breakfast!!</td>\n",
       "      <td>2015-05-23 13:33:04</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>RNupfU2QClRq_lRR3mEgrQ</td>\n",
       "      <td>5kWhgFvaf9_zeyOfO-2NxQ</td>\n",
       "      <td>Try the calamari, the chicken Marsala was grea...</td>\n",
       "      <td>2013-02-15 02:29:41</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>alUlVVMx9NtfrvK4xAQy2w</td>\n",
       "      <td>C7WUvmGAz_t4FE_ycOPGUA</td>\n",
       "      <td>Try the in house eye of round at the deli. Sli...</td>\n",
       "      <td>2015-09-06 23:00:23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>tslp3KJf3tATFHJnFzeyVA</td>\n",
       "      <td>UakVMT3xrpbFB2pHdxPjnw</td>\n",
       "      <td>This is the undisputed breakfast champion on D...</td>\n",
       "      <td>2012-10-13 14:02:43</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    user_id             business_id  \\\n",
       "24   tA1U-XSh9woo73eQmWGyAQ  xHwvbm1SJwtaZtOZzFQcmQ   \n",
       "52   nhXyjGOCfi-EW09bJqfAzQ  1Efad30BdOeqqjX2d6P4sw   \n",
       "210  RNupfU2QClRq_lRR3mEgrQ  5kWhgFvaf9_zeyOfO-2NxQ   \n",
       "332  alUlVVMx9NtfrvK4xAQy2w  C7WUvmGAz_t4FE_ycOPGUA   \n",
       "355  tslp3KJf3tATFHJnFzeyVA  UakVMT3xrpbFB2pHdxPjnw   \n",
       "\n",
       "                                                  text                 date  \\\n",
       "24   If you haven't  been here in a good while or i...  2016-06-11 23:18:23   \n",
       "52               You MUST have the popover breakfast!!  2015-05-23 13:33:04   \n",
       "210  Try the calamari, the chicken Marsala was grea...  2013-02-15 02:29:41   \n",
       "332  Try the in house eye of round at the deli. Sli...  2015-09-06 23:00:23   \n",
       "355  This is the undisputed breakfast champion on D...  2012-10-13 14:02:43   \n",
       "\n",
       "     compliment_count  \n",
       "24                  1  \n",
       "52                  1  \n",
       "210                 1  \n",
       "332                 1  \n",
       "355                 1  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load tips.json\n",
    "with open(\"/Users/vinayakgoswamy/Downloads/Yelp JSON/yelp_dataset/yelp_academic_dataset_tip.json\", \"r\") as f:\n",
    "    tip_data = [json.loads(line) for line in f]\n",
    "\n",
    "tip_df = pd.DataFrame(tip_data)\n",
    "\n",
    "# Drop rows with missing critical fields\n",
    "tip_df = tip_df.dropna(subset=[ \"date\", \"business_id\", \"user_id\"])\n",
    "\n",
    "# Filter by valid business_ids\n",
    "\n",
    "tip_df = tip_df[tip_df[\"business_id\"].isin(valid_business_ids)]\n",
    "\n",
    "tip_df = tip_df[tip_df[\"compliment_count\"] > 0]\n",
    "\n",
    "\n",
    "# Save cleaned tips\n",
    "tip_df.to_csv(\"/Users/vinayakgoswamy/Desktop/DSM Final Project/cleaned_tip.csv\", index=False)\n",
    "\n",
    "# Preview\n",
    "tip_df.head()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "users.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned user count: 1987843\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "filtered_users = []\n",
    "\n",
    "with open(\"/Users/vinayakgoswamy/Downloads/Yelp JSON/yelp_dataset/yelp_academic_dataset_user.json\", \"r\") as f:\n",
    "    for line in f:\n",
    "        user = json.loads(line)\n",
    "\n",
    "        # Check for missing user_id or name\n",
    "        if not user.get(\"user_id\") or not user.get(\"name\"):\n",
    "            continue\n",
    "\n",
    "        # Check review count\n",
    "        if user.get(\"review_count\", 0) == 0:\n",
    "            continue\n",
    "\n",
    "        \n",
    "\n",
    "        # All conditions passed\n",
    "        filtered_users.append(user)\n",
    "\n",
    "\n",
    "\n",
    "# Convert to DataFrame\n",
    "user_df = pd.DataFrame(filtered_users)\n",
    "\n",
    "\n",
    "print(\"Cleaned user count:\", len(user_df))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned user count: 621529\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>name</th>\n",
       "      <th>review_count</th>\n",
       "      <th>yelping_since</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>elite</th>\n",
       "      <th>friends</th>\n",
       "      <th>fans</th>\n",
       "      <th>...</th>\n",
       "      <th>compliment_more</th>\n",
       "      <th>compliment_profile</th>\n",
       "      <th>compliment_cute</th>\n",
       "      <th>compliment_list</th>\n",
       "      <th>compliment_note</th>\n",
       "      <th>compliment_plain</th>\n",
       "      <th>compliment_cool</th>\n",
       "      <th>compliment_funny</th>\n",
       "      <th>compliment_writer</th>\n",
       "      <th>compliment_photos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>qVc8ODYU5SZjKXVBgXdI7w</td>\n",
       "      <td>Walker</td>\n",
       "      <td>585</td>\n",
       "      <td>2007-01-25 16:47:26</td>\n",
       "      <td>7217</td>\n",
       "      <td>1259</td>\n",
       "      <td>5994</td>\n",
       "      <td>2007</td>\n",
       "      <td>NSCy54eWehBJyZdG2iE84w, pe42u7DcCH2QmI81NX-8qA...</td>\n",
       "      <td>267</td>\n",
       "      <td>...</td>\n",
       "      <td>65</td>\n",
       "      <td>55</td>\n",
       "      <td>56</td>\n",
       "      <td>18</td>\n",
       "      <td>232</td>\n",
       "      <td>844</td>\n",
       "      <td>467</td>\n",
       "      <td>467</td>\n",
       "      <td>239</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>j14WgRoU_-2ZE1aw1dXrJg</td>\n",
       "      <td>Daniel</td>\n",
       "      <td>4333</td>\n",
       "      <td>2009-01-25 04:35:42</td>\n",
       "      <td>43091</td>\n",
       "      <td>13066</td>\n",
       "      <td>27281</td>\n",
       "      <td>2009,2010,2011,2012,2013,2014,2015,2016,2017,2...</td>\n",
       "      <td>ueRPE0CX75ePGMqOFVj6IQ, 52oH4DrRvzzl8wh5UXyU0A...</td>\n",
       "      <td>3138</td>\n",
       "      <td>...</td>\n",
       "      <td>264</td>\n",
       "      <td>184</td>\n",
       "      <td>157</td>\n",
       "      <td>251</td>\n",
       "      <td>1847</td>\n",
       "      <td>7054</td>\n",
       "      <td>3131</td>\n",
       "      <td>3131</td>\n",
       "      <td>1521</td>\n",
       "      <td>1946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2WnXYQFK0hXEoTxPtV2zvg</td>\n",
       "      <td>Steph</td>\n",
       "      <td>665</td>\n",
       "      <td>2008-07-25 10:41:00</td>\n",
       "      <td>2086</td>\n",
       "      <td>1010</td>\n",
       "      <td>1003</td>\n",
       "      <td>2009,2010,2011,2012,2013</td>\n",
       "      <td>LuO3Bn4f3rlhyHIaNfTlnA, j9B4XdHUhDfTKVecyWQgyA...</td>\n",
       "      <td>52</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>66</td>\n",
       "      <td>96</td>\n",
       "      <td>119</td>\n",
       "      <td>119</td>\n",
       "      <td>35</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SZDeASXq7o05mMNLshsdIA</td>\n",
       "      <td>Gwen</td>\n",
       "      <td>224</td>\n",
       "      <td>2005-11-29 04:38:33</td>\n",
       "      <td>512</td>\n",
       "      <td>330</td>\n",
       "      <td>299</td>\n",
       "      <td>2009,2010,2011</td>\n",
       "      <td>enx1vVPnfdNUdPho6PH_wg, 4wOcvMLtU6a9Lslggq74Vg...</td>\n",
       "      <td>28</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>16</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hA5lMy-EnncsH4JoR-hFGQ</td>\n",
       "      <td>Karen</td>\n",
       "      <td>79</td>\n",
       "      <td>2007-01-05 19:40:59</td>\n",
       "      <td>29</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td></td>\n",
       "      <td>PBK4q9KEEBHhFvSXCUirIw, 3FWPpM7KU1gXeOM_ZbYMbA...</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  user_id    name  review_count        yelping_since  useful  \\\n",
       "0  qVc8ODYU5SZjKXVBgXdI7w  Walker           585  2007-01-25 16:47:26    7217   \n",
       "1  j14WgRoU_-2ZE1aw1dXrJg  Daniel          4333  2009-01-25 04:35:42   43091   \n",
       "2  2WnXYQFK0hXEoTxPtV2zvg   Steph           665  2008-07-25 10:41:00    2086   \n",
       "3  SZDeASXq7o05mMNLshsdIA    Gwen           224  2005-11-29 04:38:33     512   \n",
       "4  hA5lMy-EnncsH4JoR-hFGQ   Karen            79  2007-01-05 19:40:59      29   \n",
       "\n",
       "   funny   cool                                              elite  \\\n",
       "0   1259   5994                                               2007   \n",
       "1  13066  27281  2009,2010,2011,2012,2013,2014,2015,2016,2017,2...   \n",
       "2   1010   1003                           2009,2010,2011,2012,2013   \n",
       "3    330    299                                     2009,2010,2011   \n",
       "4     15      7                                                      \n",
       "\n",
       "                                             friends  fans  ...  \\\n",
       "0  NSCy54eWehBJyZdG2iE84w, pe42u7DcCH2QmI81NX-8qA...   267  ...   \n",
       "1  ueRPE0CX75ePGMqOFVj6IQ, 52oH4DrRvzzl8wh5UXyU0A...  3138  ...   \n",
       "2  LuO3Bn4f3rlhyHIaNfTlnA, j9B4XdHUhDfTKVecyWQgyA...    52  ...   \n",
       "3  enx1vVPnfdNUdPho6PH_wg, 4wOcvMLtU6a9Lslggq74Vg...    28  ...   \n",
       "4  PBK4q9KEEBHhFvSXCUirIw, 3FWPpM7KU1gXeOM_ZbYMbA...     1  ...   \n",
       "\n",
       "   compliment_more  compliment_profile  compliment_cute  compliment_list  \\\n",
       "0               65                  55               56               18   \n",
       "1              264                 184              157              251   \n",
       "2               13                  10               17                3   \n",
       "3                4                   1                6                2   \n",
       "4                1                   0                0                0   \n",
       "\n",
       "   compliment_note  compliment_plain  compliment_cool  compliment_funny  \\\n",
       "0              232               844              467               467   \n",
       "1             1847              7054             3131              3131   \n",
       "2               66                96              119               119   \n",
       "3               12                16               26                26   \n",
       "4                1                 1                0                 0   \n",
       "\n",
       "   compliment_writer  compliment_photos  \n",
       "0                239                180  \n",
       "1               1521               1946  \n",
       "2                 35                 18  \n",
       "3                 10                  9  \n",
       "4                  0                  0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compliment_cols = [col for col in user_df.columns if col.startswith('compliment_')]\n",
    "user_df['compliment_sum'] = user_df[compliment_cols].sum(axis=1)\n",
    "user_df = user_df[user_df['compliment_sum'] > 0]\n",
    "\n",
    "# Drop the temporary sum column\n",
    "user_df = user_df.drop(columns=['compliment_sum'])\n",
    "print(\"Cleaned user count:\", len(user_df))\n",
    "\n",
    "\n",
    "# (Optional) Save to CSV for future use\n",
    "user_df.to_csv(\"/Users/vinayakgoswamy/Desktop/DSM Final Project/cleaned_users.csv\", index=False)\n",
    "\n",
    "# Preview\n",
    "user_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned user count: 314058\n"
     ]
    }
   ],
   "source": [
    "# 3. Drop users with 0 fans\n",
    "further_user_df = user_df[user_df['fans'] > 0]\n",
    "print(\"Cleaned user count:\", len(further_user_df))\n",
    "further_user_df.to_csv(\"/Users/vinayakgoswamy/Desktop/DSM Final Project/further_cleaned_users.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
